[id="ztp-for-factory-prerequisites"]
= Prerequisites
include::modules/common-attributes.adoc[]
:context: ztp-for-factory-prerequisites

toc::[]

Installer-provisioned installation of {product-title} requires:

== Base
- OpenShift Cluster with 3 masters
    . All Cluster Operators in good health status
    . Cluster reachable via a `KUBECONFIG` file
    . The API/API-INT/Ingress should be deployed on the DHCP Ext Network (Factory network)

== Networking
- Only one physical nic is required (the NIC used for the DHCP Ext Network). In this case, the internal network will be a sub-interface from the external network using a vlan tag in the edge cluster configuration. However, you could define 2 nics in the config yaml file (internal and external), and the internal network will be another physical interface instead of using vlan tag.

- DNS entries configured and resolvable from both internal and external network, with DNS on the DHCP Factory network
    - HUB
        . `api.<hub-domain>.<domain>` and `api-int.<hub-domain>.<domain>` entries to the same IP address
        . ingress (*.apps.<hub-domain>.<net-domain>)
    - EDGE
        . `api.<edgecluster-domain>.<net-domain>` and `api-int.<edgecluster-domain>.<net-domain>` entries to the same IP address
        . ingress (*.apps.<edgecluster-domain>.<net-domain>)
- External DHCP with some free IPs on the factory to provide access to the Edge-cluster using the external network interface
    - Every Edge-cluster will need at least ~6 IPs from this External Network (without the broadcast and network IP)
        . 1 per node
        . 1 API and same for API-INT
        . 1 for the Ingress entry (*.apps.<edgecluster-domain>.<net-domain>)

== Storage
- We need some existing PVs on the HUB
    NOTE:: We cannot use `emptyDir` directive for runninng the pipeline, because between each step in the pipeline the contents will be removed and we require them to further progress.
    . 3 PVs for ACM (the expected size will depend on how many edgeclusters will you deploy)
    . 1 for the Hub Internal Registry, the base installation (which includes ACM, MetalLB, OCP version 4.X, NMState and some more images) we will need at least 900Gb on the Hub
    side (Maybe more if you have OCS/ODF deployed).
    . 1 for the HTTPD server, which will host the RHCOS images.
    . We need to meet the OpenShift Storage requirements for the Hub like (SSD/NVME).
    . LSO should be enough but we recommend to use a more reliable storage backend like ODF or NFS in order to avoid issues with the PVs and node scheduling pods.
- Create a PVC called `ztp-pvc` that will be used by the hub pipeline itself. You can use the following yaml to create the PVC.
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
  name: ztp-pvc
  namespace: edgecluster-deployer
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeMode: Filesystem
```
- In case you want to use dynamic PV and PVC creation using LVMO - we've prepared a Tekton task to do that:
```
run-hub-lvmo-task:
	tkn task start -n edgecluster-deployer \
    			-p ztp-container-image="$(PIPE_IMAGE):$(BRANCH)" \
    			-p edgeclusters-config="$$(cat $(EDGECLUSTERS_FILE))" \
    			-p kubeconfig=${KUBECONFIG} \
			    -w name=ztp,emptyDir="" \
    			--timeout 5h \
    			--use-param-defaults hub-deploy-lvmo \
    			--showlog
```
    NOTE:: You'll need to make sure that the HUB's master nodes have one available block device to form the required volume group. The total size of these block devices should be no less than 900GB.



== General
- `edgeclusters.yaml` file with the configuration for the edgeclusters (In this initial version you will need to bake this file by hand)
- The enclosure is suppose to be just one Edge-cluster which contains 3 masters, 1 worker and 1 Switch L2-L3
- The disks will be encrypted with TPMv2 so if you are using virtual environment using libvirt instead of physical servers, you will need to do this:
    . Install ```swtpm``` package
    . Configure libvirt device section to add:
     ``` <tpm model='tpm-tis'>
         <backend type='emulator' version='2.0'/>
         </tpm>
     ```

Of course, the requirements for the installation of {product-title} are also to be satisfied on the hardware involved in the installation.

== The Edge-clusters YAML file

The ```edgeclusters.yaml``` file contains all the configuration information required about the setup.

There's an example in the repo at <https://raw.githubusercontent.com/rh-ecosystem-edge/ztp-pipeline-relocatable/main/examples/config.yaml>

As you can check, it has two major sections `config` and `edgeclusters` that will be explained in the next section.

Just keep in mind that the edgeclusters section, can contain several `edgecluster-name` entries, one per edgecluster cluster to be deployed by the workflow.

=== Edge-clusters.yaml walktrough

Check next table for  a commented configuration file with links to the explanation to each relevant file section and configuration value.

[source,yaml, subs="verbatim,macros,attributes"]
ifdef::backend-pdf[[listing]]
----
ifeval::[{product-version} >= 1.0]
include::ztp-for-factory-edgeclusters-yaml.adoc[]
endif::[]
----


.Required parameters
|===
|Parameter/Section | Description


| [[config]] `config`
| This section marks the cluster configuration values that will be used for installation or configuration in both Hub and Edge-clusters.

| [[clusterimageset]] `clusterimageset`
| This setting defines the Cluster Image Set used for the HUB and the Edge-clusters

| [[OC_OCP_VERSION]] `OC_OCP_VERSION`
| Defines the OpenShift version to be used for the installation.

| [[OC_OCP_TAG]] `OC_OCP_TAG`
| This setting defines version tag to use

| [[OC_RHCOS_RELEASE]] `OC_RHCOS_RELEASE`
| This is the release to be used

| [[OC_ACM_VERSION]] `OC_ACM_VERSION`
| Specifies which ACM version should be used for the deployment

| [[OC_ODF_VERSION]] `OC_ODF_VERSION`
| This defines the ODF version to be used

| [[edgeclusters]] `edgeclusters`
| This section is the one containing the configuration for each one of the Edge-cluster Clusters


|[[edgeclustername]] `edgeclustername`
| This option is configurable and will be the name to be used for the edgecluster cluster

|[[mastername]] `mastername`
| This value must match `master0`, `master1` or `master2`.

|[[ignore_ifaces]] `ignore_ifaces`
| (Optional) Interfaces to ignore in the host

|[[nic_ext_dhcp]] `nic_ext_dhcp`
| NIC connected to the external DHCP

|[[nic_int_static]] `nic_int_static`
| NIC interface name connected to the internal network

|[[mac_ext_dhcp]] `mac_ext_dhcp`
| MAC Address for the NIC connected to the external DHCP network

|[[mac_int_static]] `mac_int_static`
| MAC Address for the NIC connected to the internal static network

|[[bmc_url]] `bmc_url`
| URL for the Baseboard Management Controller

|[[bmc_user]] `bmc_user`
| Username for the BMC

|[[bmc_pass]] `bmc_pass`
| Password for the BMC

|[[root_disk]] `root_disk`
| Mandatory: Disk device to be used for OS installation

|[[storage_disk]] `storage_disk`
| List of disk available in the node to be used for storage

|[[workername]] `workername`
| Hardcoded name as `worker0` for the worker node

|===
